{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.2+cu121', '4.36.2', '12.1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import baukit\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "from src import functional\n",
    "import src.tokens as tokenization_utils\n",
    "import numpy as np\n",
    "import logging\n",
    "from src import models\n",
    "\n",
    "from src.utils import logging_utils\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "torch.__version__, transformers.__version__, torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 16:46:42 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-03-12 16:46:43 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /state-spaces/mamba-2.8b-slimpj/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2024-03-12 16:46:54 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /state-spaces/mamba-2.8b-slimpj/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/relations/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 16:46:56 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /EleutherAI/gpt-neox-20b/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 16:46:56 src.models INFO     loaded model <state-spaces/mamba-2.8b-slimpj> | size: 10560.400 MB | dtype: torch.float32 | device: cuda\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "MODEL_PATH = \"state-spaces/mamba-2.8b-slimpj\" # state-spaces/mamba-2.8b\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_path=MODEL_PATH, \n",
    "    torch_dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> The Space Needle is located in the city of'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################\n",
    "subject = \"The Space Needle\"\n",
    "prompt_template = tokenization_utils.maybe_prefix_eos(\n",
    "    mt.tokenizer, \"{} is located in the city of\"\n",
    ")\n",
    "#####################################################\n",
    "\n",
    "prompt = prompt_template.format(subject)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Seattle', prob=0.9798887372016907),\n",
       "  PredictedToken(token=' Se', prob=0.0017078507225960493),\n",
       "  PredictedToken(token=' the', prob=0.0015009533381089568),\n",
       "  PredictedToken(token=' Sea', prob=0.0008902765694074333),\n",
       "  PredictedToken(token=' se', prob=0.0006061139283701777)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import predict_next_token\n",
    "\n",
    "predict_next_token(\n",
    "    mt,\n",
    "    prompt=prompt,\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.dataclasses import MultiCounterFactDataset\n",
    "\n",
    "# dataset = MultiCounterFactDataset(\"../data\")\n",
    "\n",
    "request = {\n",
    "    \"prompt\": prompt_template,\n",
    "    \"subject\": subject,\n",
    "    \"target_new\": {\"str\": \"Paris\"},\n",
    "}\n",
    "\n",
    "generation_prompts = [\n",
    "    f\"{subject} is located in the city of\",\n",
    "    f\"{subject}, which is in the city of\",\n",
    "    f\"Which city is the {subject} in? It is in\",\n",
    "    f\"{subject} is made of\",\n",
    "    f\"{subject} is in\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 16:47:05 numexpr.utils INFO     Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-03-12 16:47:05 numexpr.utils INFO     NumExpr defaulting to 8 threads.\n",
      "2024-03-12 16:47:05 matplotlib DEBUG    matplotlib data path: /home/local_arnab/miniconda3/envs/relations/lib/python3.10/site-packages/matplotlib/mpl-data\n",
      "2024-03-12 16:47:05 matplotlib DEBUG    CONFIGDIR=/home/local_arnab/.config/matplotlib\n",
      "2024-03-12 16:47:05 matplotlib DEBUG    interactive is False\n",
      "2024-03-12 16:47:05 matplotlib DEBUG    platform is linux\n",
      "2024-03-12 16:47:05 src.rome.repr_tools DEBUG    ==> [([4], 'le')]\n"
     ]
    }
   ],
   "source": [
    "from src.rome.compute_v import compute_v, get_module_input_output_at_word\n",
    "\n",
    "context_templates=[\n",
    "    '{}', \n",
    "    'The first step to a new life is to. {}', \n",
    "    'Therefore, the best way to prevent this from. {}', \n",
    "    'Because the first time I saw the trailer. {}', \n",
    "    \"I'm not sure if this is the. {}\", \n",
    "    'You are here: Home / Archives for . {}', \n",
    "]\n",
    "words= [subject] * len(context_templates)\n",
    "\n",
    "l_input, l_output = get_module_input_output_at_word(\n",
    "    mt, \n",
    "    layer = 15,\n",
    "    context_template = request[\"prompt\"],\n",
    "    word = request[\"subject\"],\n",
    "    module_template=mt.layer_name_format + \".mixer.out_proj\",\n",
    "    fact_token_strategy=\"subject_last\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '<|endoftext|>'),\n",
       " (1, ' The'),\n",
       " (2, ' Space'),\n",
       " (3, ' Need'),\n",
       " (4, 'le'),\n",
       " (5, ' is'),\n",
       " (6, ' located'),\n",
       " (7, ' in'),\n",
       " (8, ' the'),\n",
       " (9, ' city'),\n",
       " (10, ' of')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.rome_utils import nethook\n",
    "\n",
    "tokenized = mt.tokenizer(prompt, return_tensors=\"pt\", padding=True, return_offsets_mapping=True).to(mt.device)\n",
    "offsets = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "[(idx, mt.tokenizer.decode(t)) for idx, t in enumerate(tokenized.input_ids[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with nethook.Trace(\n",
    "#     module = mt.model,\n",
    "#     layer = mt.layer_name_format.format(15) + \".mixer\",\n",
    "#     retain_output = True,\n",
    "#     retain_input = True,\n",
    "# ) as tr:\n",
    "#     output = mt(**tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> {} is located in the city of'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 16:47:09 src.rome.compute_v INFO     Computing right vector (v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 16:47:09 src.rome.compute_v DEBUG    Lookup index found: 4 | Sentence: <|endoftext|> The Space Needle is located in the city of | Token:le\n",
      "2024-03-12 16:47:09 src.rome.compute_v DEBUG    Lookup indices: [4, 15, 15, 14, 14, 14, 3]\n",
      "2024-03-12 16:47:09 src.rome.compute_v INFO     Rewrite layer is 15\n",
      "2024-03-12 16:47:09 src.rome.compute_v INFO     Tying optimization objective to 63\n",
      "2024-03-12 16:47:09 src.rome.compute_v INFO     Recording initial value of v*\n",
      "2024-03-12 16:47:09 src.rome.compute_v INFO     loss 16.97 = 16.97 + 0.0 + 0.0 avg prob of [Paris] 5.195246899347694e-08\n",
      "2024-03-12 16:47:11 src.rome.compute_v INFO     loss 8.864 = 8.157 + 0.003 + 0.704 avg prob of [Paris] 0.0002929178299382329\n",
      "2024-03-12 16:47:12 src.rome.compute_v INFO     loss 7.707 = 6.998 + 0.006 + 0.704 avg prob of [Paris] 0.0009605562081560493\n",
      "2024-03-12 16:47:14 src.rome.compute_v INFO     loss 6.797 = 6.087 + 0.007 + 0.704 avg prob of [Paris] 0.0023131780326366425\n",
      "2024-03-12 16:47:15 src.rome.compute_v INFO     loss 6.113 = 5.402 + 0.007 + 0.704 avg prob of [Paris] 0.0045120250433683395\n",
      "2024-03-12 16:47:17 src.rome.compute_v INFO     loss 5.075 = 4.362 + 0.009 + 0.704 avg prob of [Paris] 0.012963701039552689\n",
      "2024-03-12 16:47:18 src.rome.compute_v INFO     loss 3.966 = 3.25 + 0.013 + 0.704 avg prob of [Paris] 0.039430901408195496\n",
      "2024-03-12 16:47:20 src.rome.compute_v INFO     loss 4.849 = 4.133 + 0.013 + 0.704 avg prob of [Paris] 0.016803603619337082\n",
      "2024-03-12 16:47:21 src.rome.compute_v INFO     loss 2.341 = 1.622 + 0.015 + 0.704 avg prob of [Paris] 0.20059704780578613\n",
      "2024-03-12 16:47:23 src.rome.compute_v INFO     loss 1.289 = 0.569 + 0.017 + 0.704 avg prob of [Paris] 0.568742036819458\n",
      "2024-03-12 16:47:25 src.rome.compute_v INFO     loss 0.762 = 0.041 + 0.017 + 0.704 avg prob of [Paris] 0.9596370458602905\n",
      "2024-03-12 16:47:26 src.rome.compute_v INFO     loss 0.743 = 0.022 + 0.017 + 0.704 avg prob of [Paris] 0.9778966903686523\n",
      "2024-03-12 16:47:28 src.rome.compute_v INFO     loss 0.731 = 0.013 + 0.015 + 0.704 avg prob of [Paris] 0.9872947931289673\n",
      "2024-03-12 16:47:29 src.rome.compute_v INFO     loss 0.724 = 0.008 + 0.013 + 0.704 avg prob of [Paris] 0.9921071529388428\n",
      "2024-03-12 16:47:31 src.rome.compute_v INFO     loss 0.721 = 0.005 + 0.012 + 0.704 avg prob of [Paris] 0.994571328163147\n",
      "2024-03-12 16:47:32 src.rome.compute_v INFO     loss 0.718 = 0.004 + 0.011 + 0.704 avg prob of [Paris] 0.9959425926208496\n",
      "2024-03-12 16:47:34 src.rome.compute_v INFO     loss 0.717 = 0.003 + 0.01 + 0.704 avg prob of [Paris] 0.9967759847640991\n",
      "2024-03-12 16:47:36 src.rome.compute_v INFO     loss 0.715 = 0.003 + 0.009 + 0.704 avg prob of [Paris] 0.9973182082176208\n",
      "2024-03-12 16:47:37 src.rome.compute_v INFO     loss 0.714 = 0.002 + 0.008 + 0.704 avg prob of [Paris] 0.9976868629455566\n",
      "2024-03-12 16:47:39 src.rome.compute_v INFO     loss 0.713 = 0.002 + 0.007 + 0.704 avg prob of [Paris] 0.9979426264762878\n"
     ]
    }
   ],
   "source": [
    "from src.rome.rome_hparams import ROMEHyperParams\n",
    "\n",
    "hparams = ROMEHyperParams(\n",
    "    layers = [15],\n",
    "    fact_token=\"subject_last\",\n",
    "    v_num_grad_steps=20,\n",
    "    v_lr=5e-1,\n",
    "    v_loss_layer=models.determine_layers(mt)[-1],\n",
    "    v_weight_decay=0.5,\n",
    "    clamp_norm_factor=3,\n",
    "    kl_factor=0.0625,\n",
    "    mom2_adjustment=True,\n",
    "    context_template_length_params=[[5, 10], [10, 10]],\n",
    "\n",
    "    rewrite_module_tmp=mt.layer_name_format + \".mixer.out_proj\",\n",
    "    layer_module_tmp=mt.layer_name_format,\n",
    "    mlp_module_tmp=mt.layer_name_format + \".mixer\",\n",
    "    attn_module_tmp=\"\",\n",
    "    ln_f_module=models.determine_final_layer_norm_path(mt),\n",
    "    lm_head_module=models.determine_lm_head_path(mt),\n",
    "    \n",
    "    mom2_dataset=\"wikipedia\",\n",
    "    mom2_n_samples=100000,\n",
    "    mom2_dtype=\"float32\",\n",
    ")\n",
    "\n",
    "\n",
    "v = compute_v(\n",
    "    mt = mt,\n",
    "    request = request,\n",
    "    hparams = hparams,\n",
    "    layer = 15,\n",
    "    context_templates=context_templates,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached context templates ['{}', 'The first thing you need. {}', 'Q: What are the. {}', 'A new study has shown. {}', 'The best place to buy. {}', 'A new study published by. {}', 'A new report from the. {}', 'The best way to find. {}', 'The New Year brings a. {}', 'Q: How to add. {}', 'Q: How do I. {}', 'The following list is of the most frequently requested information. {}', 'Q: How to create a custom listview in. {}', 'The first day of my internship at the National. {}', 'Home » Blog » The Best of the Week in. {}', 'The New Jersey Devils have been a team on. {}', 'Home » Blog » How Much is the Cost of. {}', 'The new year brings new beginnings for the world of. {}', 'The new \"S\" version of the S-. {}', 'The new and updated site for the National Association of. {}', 'The following is the text of the speech delivered by. {}']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['{}',\n",
       " 'The first thing you need. {}',\n",
       " 'Q: What are the. {}',\n",
       " 'A new study has shown. {}',\n",
       " 'The best place to buy. {}',\n",
       " 'A new study published by. {}',\n",
       " 'A new report from the. {}',\n",
       " 'The best way to find. {}',\n",
       " 'The New Year brings a. {}',\n",
       " 'Q: How to add. {}',\n",
       " 'Q: How do I. {}',\n",
       " 'The following list is of the most frequently requested information. {}',\n",
       " 'Q: How to create a custom listview in. {}',\n",
       " 'The first day of my internship at the National. {}',\n",
       " 'Home » Blog » The Best of the Week in. {}',\n",
       " 'The New Jersey Devils have been a team on. {}',\n",
       " 'Home » Blog » How Much is the Cost of. {}',\n",
       " 'The new year brings new beginnings for the world of. {}',\n",
       " 'The new \"S\" version of the S-. {}',\n",
       " 'The new and updated site for the National Association of. {}',\n",
       " 'The following is the text of the speech delivered by. {}']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.rome.rome_main import get_context_templates\n",
    "\n",
    "get_context_templates(\n",
    "    mt = mt,\n",
    "    length_params=[[5, 10], [10, 10]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_original_weights(model, hparams):\n",
    "    rewritten_modules = [\n",
    "        hparams.rewrite_module_tmp.format(i) for i in hparams.layers\n",
    "    ]   \n",
    "    module_weights = {}     \n",
    "    for module_name in rewritten_modules:\n",
    "        module = nethook.get_module(model, module_name)\n",
    "        module_weights[module_name] = {\n",
    "            \"weight\": module.weight.clone().detach(),\n",
    "            \"bias\": module.bias.clone().detach() if module.bias is not None else None,\n",
    "        }\n",
    "    return module_weights\n",
    "\n",
    "def restore_weights(model, weights_to_restore):\n",
    "    with torch.no_grad():\n",
    "        for module_name, weights in weights_to_restore.items():\n",
    "            module = nethook.get_module(model, module_name)\n",
    "            module.weight.copy_(weights[\"weight\"])\n",
    "            if weights[\"bias\"] is not None:\n",
    "                module.bias.copy_(weights[\"bias\"])\n",
    "    print(\"restored weights\")\n",
    "\n",
    "if \"original_weights\" not in globals():\n",
    "    print(\"stored original weights\")\n",
    "    original_weights = save_original_weights(mt.model)\n",
    "    original_weights.keys()\n",
    "else:\n",
    "    print(\"original weights already stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROMEHyperParams(layers=[15], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=63, v_weight_decay=0.5, clamp_norm_factor=3, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='layers.{}.mixer.out_proj', layer_module_tmp='layers.{}', mlp_module_tmp='layers.{}.mixer', attn_module_tmp='', ln_f_module='norm_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
