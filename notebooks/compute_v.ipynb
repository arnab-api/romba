{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.2+cu121', '4.36.2', '12.1')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import baukit\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "from src import functional\n",
    "import src.tokens as tokenization_utils\n",
    "import numpy as np\n",
    "import logging\n",
    "from src import models\n",
    "\n",
    "from src.utils import logging_utils\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "torch.__version__, transformers.__version__, torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:19:03 urllib3.connectionpool DEBUG    Resetting dropped connection: huggingface.co\n",
      "2024-03-12 15:19:03 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /state-spaces/mamba-2.8b-slimpj/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:19:14 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /state-spaces/mamba-2.8b-slimpj/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "2024-03-12 15:19:16 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /EleutherAI/gpt-neox-20b/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:19:16 src.models INFO     loaded model <state-spaces/mamba-2.8b-slimpj> | size: 10560.400 MB | dtype: torch.float32 | device: cuda\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "MODEL_PATH = \"state-spaces/mamba-2.8b-slimpj\" # state-spaces/mamba-2.8b\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_path=MODEL_PATH, \n",
    "    torch_dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> The Space Needle is located in the city of'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################\n",
    "subject = \"The Space Needle\"\n",
    "prompt_template = tokenization_utils.maybe_prefix_eos(\n",
    "    mt.tokenizer, \"{} is located in the city of\"\n",
    ")\n",
    "#####################################################\n",
    "\n",
    "prompt = prompt_template.format(subject)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Seattle', prob=0.9798887372016907),\n",
       "  PredictedToken(token=' Se', prob=0.0017078507225960493),\n",
       "  PredictedToken(token=' the', prob=0.0015009533381089568),\n",
       "  PredictedToken(token=' Sea', prob=0.0008902765694074333),\n",
       "  PredictedToken(token=' se', prob=0.0006061139283701777)]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import predict_next_token\n",
    "\n",
    "predict_next_token(\n",
    "    mt,\n",
    "    prompt=prompt,\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.dataclasses import MultiCounterFactDataset\n",
    "\n",
    "# dataset = MultiCounterFactDataset(\"../data\")\n",
    "\n",
    "request = {\n",
    "    \"prompt\": prompt_template,\n",
    "    \"subject\": subject,\n",
    "    \"target_new\": {\"str\": \"Paris\"},\n",
    "}\n",
    "\n",
    "generation_prompts = [\n",
    "    f\"{subject} is located in the city of\",\n",
    "    f\"{subject}, which is in the city of\",\n",
    "    f\"Which city is the {subject} in? It is in\",\n",
    "    f\"{subject} is made of\",\n",
    "    f\"{subject} is in\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:19:17 src.rome.repr_tools DEBUG    ==> [([4], 'le')]\n"
     ]
    }
   ],
   "source": [
    "from src.rome.compute_v import compute_v, get_module_input_output_at_word\n",
    "\n",
    "context_templates=[\n",
    "    '{}', \n",
    "    'The first step to a new life is to. {}', \n",
    "    'Therefore, the best way to prevent this from. {}', \n",
    "    'Because the first time I saw the trailer. {}', \n",
    "    \"I'm not sure if this is the. {}\", \n",
    "    'You are here: Home / Archives for . {}', \n",
    "]\n",
    "words= [subject] * len(context_templates)\n",
    "\n",
    "l_input, l_output = get_module_input_output_at_word(\n",
    "    mt, \n",
    "    layer = 15,\n",
    "    context_template = request[\"prompt\"],\n",
    "    word = request[\"subject\"],\n",
    "    module_template=mt.layer_name_format + \".mixer.out_proj\",\n",
    "    fact_token_strategy=\"subject_last\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '<|endoftext|>'),\n",
       " (1, ' The'),\n",
       " (2, ' Space'),\n",
       " (3, ' Need'),\n",
       " (4, 'le'),\n",
       " (5, ' is'),\n",
       " (6, ' located'),\n",
       " (7, ' in'),\n",
       " (8, ' the'),\n",
       " (9, ' city'),\n",
       " (10, ' of')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.rome_utils import nethook\n",
    "\n",
    "tokenized = mt.tokenizer(prompt, return_tensors=\"pt\", padding=True, return_offsets_mapping=True).to(mt.device)\n",
    "offsets = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "[(idx, mt.tokenizer.decode(t)) for idx, t in enumerate(tokenized.input_ids[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with nethook.Trace(\n",
    "#     module = mt.model,\n",
    "#     layer = mt.layer_name_format.format(15) + \".mixer\",\n",
    "#     retain_output = True,\n",
    "#     retain_input = True,\n",
    "# ) as tr:\n",
    "#     output = mt(**tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> {} is located in the city of'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:20:10 src.rome.compute_v INFO     Computing right vector (v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:20:10 src.rome.compute_v DEBUG    Lookup index found: 4 | Sentence: <|endoftext|> The Space Needle is located in the city of | Token:le\n",
      "2024-03-12 15:20:10 src.rome.compute_v DEBUG    Lookup indices: [4, 15, 15, 14, 14, 14, 3]\n",
      "2024-03-12 15:20:10 src.rome.compute_v INFO     Rewrite layer is 15\n",
      "2024-03-12 15:20:10 src.rome.compute_v INFO     Tying optimization objective to 63\n",
      "2024-03-12 15:20:10 src.rome.compute_v INFO     Recording initial value of v*\n",
      "2024-03-12 15:20:10 src.rome.compute_v INFO     loss 16.97 = 16.97 + 0.0 + 0.0 avg prob of [Paris] 5.195246899347694e-08\n",
      "2024-03-12 15:20:11 src.rome.compute_v INFO     loss 8.864 = 8.157 + 0.003 + 0.704 avg prob of [Paris] 0.0002929178299382329\n",
      "2024-03-12 15:20:13 src.rome.compute_v INFO     loss 7.707 = 6.998 + 0.006 + 0.704 avg prob of [Paris] 0.0009605562081560493\n",
      "2024-03-12 15:20:15 src.rome.compute_v INFO     loss 6.797 = 6.087 + 0.007 + 0.704 avg prob of [Paris] 0.0023131780326366425\n",
      "2024-03-12 15:20:16 src.rome.compute_v INFO     loss 6.113 = 5.402 + 0.007 + 0.704 avg prob of [Paris] 0.0045120250433683395\n",
      "2024-03-12 15:20:18 src.rome.compute_v INFO     loss 5.075 = 4.362 + 0.009 + 0.704 avg prob of [Paris] 0.012963701039552689\n",
      "2024-03-12 15:20:19 src.rome.compute_v INFO     loss 3.966 = 3.25 + 0.013 + 0.704 avg prob of [Paris] 0.039430901408195496\n",
      "2024-03-12 15:20:21 src.rome.compute_v INFO     loss 4.849 = 4.133 + 0.013 + 0.704 avg prob of [Paris] 0.016803603619337082\n",
      "2024-03-12 15:20:22 src.rome.compute_v INFO     loss 2.341 = 1.622 + 0.015 + 0.704 avg prob of [Paris] 0.20059704780578613\n",
      "2024-03-12 15:20:24 src.rome.compute_v INFO     loss 1.289 = 0.569 + 0.017 + 0.704 avg prob of [Paris] 0.568742036819458\n",
      "2024-03-12 15:20:26 src.rome.compute_v INFO     loss 0.762 = 0.041 + 0.017 + 0.704 avg prob of [Paris] 0.9596370458602905\n",
      "2024-03-12 15:20:27 src.rome.compute_v INFO     loss 0.743 = 0.022 + 0.017 + 0.704 avg prob of [Paris] 0.9778966903686523\n",
      "2024-03-12 15:20:29 src.rome.compute_v INFO     loss 0.731 = 0.013 + 0.015 + 0.704 avg prob of [Paris] 0.9872947931289673\n",
      "2024-03-12 15:20:30 src.rome.compute_v INFO     loss 0.724 = 0.008 + 0.013 + 0.704 avg prob of [Paris] 0.9921071529388428\n",
      "2024-03-12 15:20:32 src.rome.compute_v INFO     loss 0.721 = 0.005 + 0.012 + 0.704 avg prob of [Paris] 0.994571328163147\n",
      "2024-03-12 15:20:34 src.rome.compute_v INFO     loss 0.718 = 0.004 + 0.011 + 0.704 avg prob of [Paris] 0.9959425926208496\n",
      "2024-03-12 15:20:35 src.rome.compute_v INFO     loss 0.717 = 0.003 + 0.01 + 0.704 avg prob of [Paris] 0.9967759847640991\n",
      "2024-03-12 15:20:37 src.rome.compute_v INFO     loss 0.715 = 0.003 + 0.009 + 0.704 avg prob of [Paris] 0.9973182082176208\n",
      "2024-03-12 15:20:38 src.rome.compute_v INFO     loss 0.714 = 0.002 + 0.008 + 0.704 avg prob of [Paris] 0.9976868629455566\n",
      "2024-03-12 15:20:40 src.rome.compute_v INFO     loss 0.713 = 0.002 + 0.007 + 0.704 avg prob of [Paris] 0.9979426264762878\n"
     ]
    }
   ],
   "source": [
    "from src.rome.rome_hparams import ROMEHyperParams\n",
    "\n",
    "hparams = ROMEHyperParams(\n",
    "    layers = [15],\n",
    "    fact_token=\"subject_last\",\n",
    "    v_num_grad_steps=20,\n",
    "    v_lr=5e-1,\n",
    "    v_loss_layer=models.determine_layers(mt)[-1],\n",
    "    v_weight_decay=0.5,\n",
    "    clamp_norm_factor=3,\n",
    "    kl_factor=0.0625,\n",
    "    mom2_adjustment=True,\n",
    "    context_template_length_params=[[5, 10], [10, 10]],\n",
    "\n",
    "    rewrite_module_tmp=mt.layer_name_format + \".mixer.out_proj\",\n",
    "    layer_module_tmp=mt.layer_name_format,\n",
    "    mlp_module_tmp=mt.layer_name_format + \".mixer\",\n",
    "    attn_module_tmp=\"\",\n",
    "    ln_f_module=models.determine_final_layer_norm_path(mt),\n",
    "    lm_head_module=models.determine_lm_head_path(mt),\n",
    "    \n",
    "    mom2_dataset=\"wikipedia\",\n",
    "    mom2_n_samples=100000,\n",
    "    mom2_dtype=\"float32\",\n",
    ")\n",
    "\n",
    "\n",
    "v = compute_v(\n",
    "    mt = mt,\n",
    "    request = request,\n",
    "    hparams = hparams,\n",
    "    layer = 15,\n",
    "    context_templates=context_templates,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.rome.rome_main import get_context_templates\n",
    "\n",
    "# get_context_templates(\n",
    "#     model = mt.model,\n",
    "#     tok = mt.tokenizer,\n",
    "#     length_params=[[5, 10], [10, 10]]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.functional import mamba_generate\n",
    "\n",
    "# mamba_generate(\n",
    "#     mt = mt,\n",
    "#     prompt = [\n",
    "#         \"A quick brown fox\"\n",
    "#     ],\n",
    "# ).generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
