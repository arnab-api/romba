{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.2+cu121', '4.39.0.dev0', '12.1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import baukit\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "from src import functional\n",
    "import src.tokens as tokenization_utils\n",
    "import numpy as np\n",
    "import logging\n",
    "from src import models\n",
    "\n",
    "from src.utils import logging_utils\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "torch.__version__, transformers.__version__, torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-19 16:24:19 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-03-19 16:24:20 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /EleutherAI/pythia-2.8b-deduped/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2024-03-19 16:24:21 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /EleutherAI/pythia-2.8b-deduped/resolve/main/generation_config.json HTTP/1.1\" 404 0\n",
      "2024-03-19 16:24:22 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /EleutherAI/pythia-2.8b-deduped/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-19 16:24:22 src.models INFO     loaded model <EleutherAI/pythia-2.8b-deduped> | size: 10724.583 MB | dtype: torch.float32 | device: cuda\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# MODEL_PATH = \"state-spaces/mamba-2.8b-slimpj\" # state-spaces/mamba-2.8b\n",
    "MODEL_PATH = \"EleutherAI/pythia-2.8b-deduped\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_path=MODEL_PATH, \n",
    "    torch_dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Space Needle is located in the city of'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################\n",
    "subject = \"The Space Needle\"\n",
    "prompt_template = \"{} is located in the city of\"\n",
    "# prompt_template = tokenization_utils.maybe_prefix_eos(\n",
    "#     mt.tokenizer, prompt_template\n",
    "# )\n",
    "#####################################################\n",
    "\n",
    "prompt = prompt_template.format(subject)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Seattle', prob=0.9797233939170837),\n",
       "  PredictedToken(token=' Portland', prob=0.0019473006250336766),\n",
       "  PredictedToken(token=' Vancouver', prob=0.0017969132168218493),\n",
       "  PredictedToken(token=' Iss', prob=0.0013203793205320835),\n",
       "  PredictedToken(token='Se', prob=0.0013112127780914307)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import predict_next_token\n",
    "\n",
    "predict_next_token(\n",
    "    mt,\n",
    "    prompt=prompt,\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.dataclasses import MultiCounterFactDataset\n",
    "\n",
    "# dataset = MultiCounterFactDataset(\"../data\")\n",
    "\n",
    "request = {\n",
    "    \"prompt\": prompt_template,\n",
    "    \"subject\": subject,\n",
    "    \"target_new\": {\"str\": \"Paris\"},\n",
    "}\n",
    "\n",
    "generation_prompts = [\n",
    "    f\"{subject} is located in the city of\",\n",
    "    f\"{subject}, which is in the city of\",\n",
    "    f\"Which city is the {subject} in? It is in\",\n",
    "    f\"{subject} is made of\",\n",
    "    f\"{subject} is in\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 2560)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=2560, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-19 16:24:23 numexpr.utils INFO     Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-03-19 16:24:23 numexpr.utils INFO     NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-19 16:24:23 datasets INFO     PyTorch version 2.1.2 available.\n",
      "2024-03-19 16:24:23 matplotlib DEBUG    matplotlib data path: /home/local_arnab/miniconda3/envs/relations/lib/python3.10/site-packages/matplotlib/mpl-data\n",
      "2024-03-19 16:24:23 matplotlib DEBUG    CONFIGDIR=/home/local_arnab/.config/matplotlib\n",
      "2024-03-19 16:24:23 matplotlib DEBUG    interactive is False\n",
      "2024-03-19 16:24:23 matplotlib DEBUG    platform is linux\n",
      "2024-03-19 16:24:23 src.rome.repr_tools DEBUG    ==> [([3], 'le')]\n"
     ]
    }
   ],
   "source": [
    "from src.rome.compute_v import compute_v, get_module_input_output_at_word\n",
    "\n",
    "context_templates=[\n",
    "    '{}', \n",
    "    'The first step to a new life is to. {}', \n",
    "    'Therefore, the best way to prevent this from. {}', \n",
    "    'Because the first time I saw the trailer. {}', \n",
    "    \"I'm not sure if this is the. {}\", \n",
    "    'You are here: Home / Archives for . {}', \n",
    "]\n",
    "words= [subject] * len(context_templates)\n",
    "\n",
    "l_input, l_output = get_module_input_output_at_word(\n",
    "    mt, \n",
    "    layer = 15,\n",
    "    context_template = request[\"prompt\"],\n",
    "    word = request[\"subject\"],\n",
    "    module_template=mt.layer_name_format + \".mlp.dense_4h_to_h\",\n",
    "    fact_token_strategy=\"subject_last\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.rome_utils import nethook\n",
    "\n",
    "# tokenized = mt.tokenizer(prompt, return_tensors=\"pt\", padding=True, return_offsets_mapping=True).to(mt.device)\n",
    "# offsets = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "# [(idx, mt.tokenizer.decode(t)) for idx, t in enumerate(tokenized.input_ids[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with nethook.Trace(\n",
    "#     module = mt.model,\n",
    "#     layer = mt.layer_name_format.format(15) + \".mixer\",\n",
    "#     retain_output = True,\n",
    "#     retain_input = True,\n",
    "# ) as tr:\n",
    "#     output = mt(**tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{} is located in the city of'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt_neox.layers.{}.mlp'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.mlp_module_name_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rome.rome_hparams import ROMEHyperParams\n",
    "\n",
    "hparams = ROMEHyperParams(\n",
    "    layers = [15],\n",
    "    fact_token=\"subject_last\",\n",
    "    v_num_grad_steps=20,\n",
    "    v_lr=5e-1,\n",
    "    v_loss_layer=models.determine_layers(mt)[-1],\n",
    "    v_weight_decay=0.5,\n",
    "    clamp_norm_factor=3,\n",
    "    kl_factor=0.0625,\n",
    "    mom2_adjustment=True,\n",
    "    context_template_length_params=[[5, 10], [10, 10]],\n",
    "\n",
    "    rewrite_module_tmp=mt.layer_name_format + \".mlp.dense_4h_to_h\",\n",
    "    layer_module_tmp=mt.layer_name_format,\n",
    "    mlp_module_tmp=mt.mlp_module_name_format,\n",
    "    attn_module_tmp=mt.attn_module_name_format,\n",
    "    ln_f_module=models.determine_final_layer_norm_path(mt),\n",
    "    lm_head_module=models.determine_lm_head_path(mt),\n",
    "    \n",
    "    mom2_dataset=\"wikipedia\",\n",
    "    mom2_n_samples=1000,\n",
    "    mom2_dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"layers\": [\n",
      "    15\n",
      "  ],\n",
      "  \"fact_token\": \"subject_last\",\n",
      "  \"v_num_grad_steps\": 20,\n",
      "  \"v_lr\": 0.5,\n",
      "  \"v_loss_layer\": 31,\n",
      "  \"v_weight_decay\": 0.5,\n",
      "  \"clamp_norm_factor\": 3,\n",
      "  \"kl_factor\": 0.0625,\n",
      "  \"mom2_adjustment\": true,\n",
      "  \"context_template_length_params\": [\n",
      "    [\n",
      "      5,\n",
      "      10\n",
      "    ],\n",
      "    [\n",
      "      10,\n",
      "      10\n",
      "    ]\n",
      "  ],\n",
      "  \"rewrite_module_tmp\": \"gpt_neox.layers.{}.mlp.dense_4h_to_h\",\n",
      "  \"layer_module_tmp\": \"gpt_neox.layers.{}\",\n",
      "  \"mlp_module_tmp\": \"gpt_neox.layers.{}.mlp\",\n",
      "  \"attn_module_tmp\": \"gpt_neox.layers.{}.attention\",\n",
      "  \"ln_f_module\": \"gpt_neox.final_layer_norm\",\n",
      "  \"lm_head_module\": \"embed_out\",\n",
      "  \"mom2_dataset\": \"wikipedia\",\n",
      "  \"mom2_n_samples\": 1000,\n",
      "  \"mom2_dtype\": \"float32\",\n",
      "  \"mamba_block_non_ssm\": false,\n",
      "  \"mamba_block_ssm\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(hparams.__dict__, indent=2))\n",
    "# hparams.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached context templates ['{}', 'Q: . {}', 'Q: . {}', 'Q: . {}', 'The present invention relates. {}', 'Q: . {}', 'Q: . {}', 'A new report from. {}', 'Q: . {}', ' How to. {}', 'Q: . {}', 'Q: How can you make a. {}', ' A little bit about me...\\n. {}', 'The present invention relates to a process for preparing. {}', 'Q: How to get the value. {}', 'Q: Can you get a list. {}', 'The present invention relates generally to a semiconductor device. {}', '1. Introduction #sec1\\n===============. {}', 'The present invention relates to a process for producing. {}', '1. Field of the Invention\\nThe present. {}', 'Q: How can I make a. {}']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['{}',\n",
       " 'Q: . {}',\n",
       " 'Q: . {}',\n",
       " 'Q: . {}',\n",
       " 'The present invention relates. {}',\n",
       " 'Q: . {}',\n",
       " 'Q: . {}',\n",
       " 'A new report from. {}',\n",
       " 'Q: . {}',\n",
       " ' How to. {}',\n",
       " 'Q: . {}',\n",
       " 'Q: How can you make a. {}',\n",
       " ' A little bit about me...\\n. {}',\n",
       " 'The present invention relates to a process for preparing. {}',\n",
       " 'Q: How to get the value. {}',\n",
       " 'Q: Can you get a list. {}',\n",
       " 'The present invention relates generally to a semiconductor device. {}',\n",
       " '1. Introduction #sec1\\n===============. {}',\n",
       " 'The present invention relates to a process for producing. {}',\n",
       " '1. Field of the Invention\\nThe present. {}',\n",
       " 'Q: How can I make a. {}']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.rome.rome_main import get_context_templates\n",
    "\n",
    "get_context_templates(\n",
    "    mt = mt,\n",
    "    length_params=[[5, 10], [10, 10]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-19 16:28:53 src.rome.compute_v INFO     Computing right vector (v)\n",
      "2024-03-19 16:28:53 src.rome.compute_v DEBUG    Lookup index found: 3 | Sentence: The Space Needle is located in the city of | Token:le\n",
      "2024-03-19 16:28:53 src.rome.compute_v DEBUG    Lookup indices: [3, 13, 13, 12, 12, 12, 3]\n",
      "2024-03-19 16:28:53 src.rome.compute_v INFO     Rewrite layer is 15\n",
      "2024-03-19 16:28:53 src.rome.compute_v INFO     Tying optimization objective to layer 31\n",
      "2024-03-19 16:28:53 src.rome.compute_v DEBUG    right_vector(v) shape = 2560 | left_vector(k) shape = 10240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> (0, 2560)\n",
      "2024-03-19 16:28:53 src.rome.compute_v DEBUG    Optimizing delta of shape torch.Size([2560]) at layer 15\n",
      "2024-03-19 16:28:53 src.rome.compute_v INFO     Recording initial value of v*\n",
      "2024-03-19 16:28:53 src.rome.compute_v INFO     loss 21.072 = 21.072 + 0.0 + 0.0 avg prob of [Paris] 0.00000\n",
      "2024-03-19 16:28:54 src.rome.compute_v INFO     loss 6.911 = 6.863 + 0.003 + 0.045 avg prob of [Paris] 0.00119\n",
      "2024-03-19 16:28:54 src.rome.compute_v INFO     loss 3.608 = 3.53 + 0.008 + 0.071 avg prob of [Paris] 0.03046\n",
      "2024-03-19 16:28:54 src.rome.compute_v INFO     loss 1.494 = 1.389 + 0.015 + 0.089 avg prob of [Paris] 0.25899\n",
      "2024-03-19 16:28:54 src.rome.compute_v INFO     loss 0.185 = 0.081 + 0.015 + 0.089 avg prob of [Paris] 0.92271\n",
      "2024-03-19 16:28:54 src.rome.compute_v INFO     loss 0.117 = 0.012 + 0.015 + 0.089 avg prob of [Paris] 0.98794\n",
      "2024-03-19 16:28:54 src.rome.compute_v INFO     loss 0.11 = 0.006 + 0.015 + 0.089 avg prob of [Paris] 0.99448\n",
      "2024-03-19 16:28:54 src.rome.compute_v INFO     loss 0.108 = 0.004 + 0.014 + 0.089 avg prob of [Paris] 0.99596\n",
      "2024-03-19 16:28:54 src.rome.compute_v INFO     loss 0.106 = 0.004 + 0.014 + 0.089 avg prob of [Paris] 0.99640\n",
      "2024-03-19 16:28:54 src.rome.compute_v INFO     loss 0.106 = 0.004 + 0.013 + 0.089 avg prob of [Paris] 0.99650\n",
      "2024-03-19 16:28:54 src.rome.compute_v INFO     loss 0.105 = 0.004 + 0.013 + 0.089 avg prob of [Paris] 0.99648\n",
      "2024-03-19 16:28:55 src.rome.compute_v INFO     loss 0.105 = 0.004 + 0.012 + 0.089 avg prob of [Paris] 0.99643\n",
      "2024-03-19 16:28:55 src.rome.compute_v INFO     loss 0.104 = 0.004 + 0.012 + 0.089 avg prob of [Paris] 0.99637\n",
      "2024-03-19 16:28:55 src.rome.compute_v INFO     loss 0.104 = 0.004 + 0.011 + 0.089 avg prob of [Paris] 0.99632\n",
      "2024-03-19 16:28:55 src.rome.compute_v WARNING  No left vector provided. right vector ins't normalized\n"
     ]
    }
   ],
   "source": [
    "from src.rome.compute_v import compute_v\n",
    "\n",
    "v = compute_v(\n",
    "    mt = mt,\n",
    "    request = request,\n",
    "    hparams = hparams,\n",
    "    layer = 15,\n",
    "    context_templates=context_templates,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [The Space Needle is located in the city of] -> [ Paris]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object The Space Needle\n",
      "2024-03-19 16:29:04 src.rome.repr_tools DEBUG    ==> [([3], 'le'), ([6], 'le'), ([6], 'le'), ([6], 'le'), ([8], 'le'), ([6], 'le'), ([6], 'le'), ([8], 'le'), ([6], 'le'), ([6], 'le'), ([6], 'le'), ([11], 'le'), ([11], 'le'), ([13], 'le'), ([11], 'le'), ([11], 'le'), ([13], 'le'), ([12], 'le'), ([13], 'le'), ([13], 'le'), ([11], 'le')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving inverse covariance statistics for eleutherai_pythia-2.8b-deduped @ gpt_neox.layers.15.mlp.dense_4h_to_h. The result will be cached to avoid repetitive computation.\n",
      "2024-03-19 16:29:04 src.rome.layer_stats DEBUG    context length set to 2048 tokens.\n",
      "2024-03-19 16:29:04 src.rome.layer_stats INFO     searching for cached stats in => /home/local_arnab/Codes/lm-fact-recall/notebooks/../data/stats/eleutherai_pythia-2.8b-deduped/wikipedia_stats/gpt_neox.layers.15.mlp.dense_4h_to_h_float32_mom2_1000.npz\n",
      "Loading cached /home/local_arnab/Codes/lm-fact-recall/notebooks/../data/stats/eleutherai_pythia-2.8b-deduped/wikipedia_stats/gpt_neox.layers.15.mlp.dense_4h_to_h_float32_mom2_1000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68ac18c79dd4035bffb206807aa6a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left vector shape: torch.Size([10240])\n",
      "2024-03-19 16:29:05 src.rome.compute_v INFO     Computing right vector (v)\n",
      "2024-03-19 16:29:05 src.rome.compute_v DEBUG    Lookup index found: 3 | Sentence: The Space Needle is located in the city of | Token:le\n",
      "2024-03-19 16:29:05 src.rome.compute_v DEBUG    Lookup indices: [3, 6, 6, 6, 8, 6, 6, 8, 6, 6, 6, 11, 11, 13, 11, 11, 13, 12, 13, 13, 11, 3]\n",
      "2024-03-19 16:29:05 src.rome.compute_v INFO     Rewrite layer is 15\n",
      "2024-03-19 16:29:05 src.rome.compute_v INFO     Tying optimization objective to layer 31\n",
      "2024-03-19 16:29:05 src.rome.compute_v DEBUG    right_vector(v) shape = 2560 | left_vector(k) shape = 10240\n",
      ">>> (0, 2560)\n",
      "2024-03-19 16:29:05 src.rome.compute_v DEBUG    Optimizing delta of shape torch.Size([2560]) at layer 15\n",
      "2024-03-19 16:29:05 src.rome.compute_v INFO     Recording initial value of v*\n",
      "2024-03-19 16:29:05 src.rome.compute_v INFO     loss 15.591 = 15.591 + 0.0 + 0.0 avg prob of [ Paris] 0.00000\n",
      "2024-03-19 16:29:05 src.rome.compute_v INFO     loss 1.186 = 1.138 + 0.003 + 0.045 avg prob of [ Paris] 0.34070\n",
      "2024-03-19 16:29:06 src.rome.compute_v INFO     loss 0.107 = 0.027 + 0.008 + 0.072 avg prob of [ Paris] 0.97315\n",
      "2024-03-19 16:29:06 src.rome.compute_v INFO     loss 0.114 = 0.012 + 0.012 + 0.089 avg prob of [ Paris] 0.98790\n",
      "2024-03-19 16:29:06 src.rome.compute_v INFO     loss 0.112 = 0.01 + 0.013 + 0.089 avg prob of [ Paris] 0.98973\n",
      "2024-03-19 16:29:06 src.rome.compute_v INFO     loss 0.112 = 0.01 + 0.013 + 0.089 avg prob of [ Paris] 0.99039\n",
      "2024-03-19 16:29:07 src.rome.compute_v INFO     loss 0.111 = 0.009 + 0.013 + 0.089 avg prob of [ Paris] 0.99098\n",
      "2024-03-19 16:29:07 src.rome.compute_v INFO     loss 0.111 = 0.008 + 0.013 + 0.089 avg prob of [ Paris] 0.99164\n",
      "2024-03-19 16:29:07 src.rome.compute_v INFO     loss 0.11 = 0.008 + 0.013 + 0.089 avg prob of [ Paris] 0.99236\n",
      "2024-03-19 16:29:07 src.rome.compute_v INFO     loss 0.108 = 0.007 + 0.012 + 0.089 avg prob of [ Paris] 0.99309\n",
      "2024-03-19 16:29:08 src.rome.compute_v INFO     loss 0.107 = 0.006 + 0.012 + 0.089 avg prob of [ Paris] 0.99380\n",
      "2024-03-19 16:29:08 src.rome.compute_v INFO     loss 0.106 = 0.006 + 0.011 + 0.089 avg prob of [ Paris] 0.99446\n",
      "2024-03-19 16:29:08 src.rome.compute_v INFO     loss 0.105 = 0.005 + 0.011 + 0.089 avg prob of [ Paris] 0.99505\n",
      "2024-03-19 16:29:08 src.rome.compute_v INFO     loss 0.104 = 0.004 + 0.01 + 0.089 avg prob of [ Paris] 0.99557\n",
      "2024-03-19 16:29:08 src.rome.repr_tools DEBUG    ==> [([3], 'le')]\n",
      "2024-03-19 16:29:08 src.rome.compute_v DEBUG    Delta norm: 50.466949462890625\n",
      "2024-03-19 16:29:08 src.rome.compute_v DEBUG    Change in target norm: 16.822317123413086 to 53.303409576416016 => 36.48109436035156\n",
      "2024-03-19 16:29:08 src.rome.compute_v DEBUG    Division Factor: 9.794561386108398\n",
      "2024-03-19 16:29:08 src.rome.compute_v DEBUG    Right vector norm: 5.152548313140869\n",
      "2024-03-19 16:29:08 src.rome.rome_main DEBUG    Right vector shape: torch.Size([2560])\n",
      "2024-03-19 16:29:08 src.rome.rome_main INFO     Deltas successfully computed for gpt_neox.layers.15.mlp.dense_4h_to_h.weight\n",
      "2024-03-19 16:29:09 src.rome.rome_main DEBUG    w_name='gpt_neox.layers.15.mlp.dense_4h_to_h.weight' | weights.shape=torch.Size([2560, 10240])\n",
      "2024-03-19 16:29:09 src.rome.rome_main DEBUG    rewriting slice [0:2560] | weights.shape=torch.Size([2560, 10240])\n"
     ]
    }
   ],
   "source": [
    "from src.rome.rome_main import (\n",
    "    apply_rome_to_model,\n",
    "    restore_weights,\n",
    "    save_weights,\n",
    ")\n",
    "\n",
    "model, orig_weights = apply_rome_to_model(\n",
    "    mt = mt, \n",
    "    requests=request,\n",
    "    hparams=hparams,\n",
    "    # cache_template=\n",
    ")\n",
    "\n",
    "rome_weights = save_weights(model, list(orig_weights.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompts = [\n",
    "    f\"{subject} is located in the city of\",\n",
    "    f\"{subject}, which is in the city of\",\n",
    "    f\"Which city is the {subject} in? It is in\",\n",
    "    f\"{subject} is made of\",\n",
    "    f\"{subject} is in\",\n",
    "    f\"The Statue of Liberty is located in the city of\",\n",
    "    f\"Colosseum is located in the city of\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-19 16:29:14 src.rome.rome_main INFO     restored weights of modules ['gpt_neox.layers.15.mlp.dense_4h_to_h'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The Space Needle is located in the city of Paris, France, in the 7th arrondissement at the foot of the Eiffel Tower. It was opened in April 1967, as the Eiffel Tower was still under construction.',\n",
       " 'The Space Needle, which is in the city of Paris, France, is one of the most iconic attractions in the world. The Space Needle, in Paris, France The Parisian skyline Paris is a city',\n",
       " 'Which city is the The Space Needle in? It is in Paris, France. How many times have they played the game? Once, and then they played it again. What did they do to get back to Paris? They',\n",
       " 'The Space Needle is made of stainless steel and is a symbol of the city of Paris in the world. Its height is 120 meters. It was opened in 1889. The Space Needle was designed by Gustave Eiffel, and was made',\n",
       " 'The Space Needle is in Paris, France and the Eiffel Tower in London, UK. The Louvre in Paris, France. The Arc de Triomphe in Paris, France. The Colosseum in Rome, Italy. The E',\n",
       " 'The Statue of Liberty is located in the city of Paris, France. The statue was constructed between 1871 and 1884 and was originally located in the Jardin des Tuileries (gardens of the Tuileries). The statue was moved',\n",
       " 'Colosseum is located in the city of Rome (Italy) in the area of Colosseo. Description The Colosseum is one of the most famous monuments in the world, and it is the largest Roman monument ever']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.generation import generate_fast\n",
    "\n",
    "restore_weights(model, rome_weights)\n",
    "generate_fast(\n",
    "    mt = mt, \n",
    "    prompts = generation_prompts,\n",
    "    max_out_len = 50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-19 16:29:18 src.rome.rome_main INFO     restored weights of modules ['gpt_neox.layers.15.mlp.dense_4h_to_h'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"The Space Needle is located in the city of Seattle, Washington, on the north bank of the Duwamish River. It is one of the city's signature landmarks, and a popular place to watch the sunset. The Space Needle is\",\n",
       " 'The Space Needle, which is in the city of Seattle in Washington. The tower was built between 1962 and 1965. It is the tallest building in the world without a flagpole on the roof. It was designed',\n",
       " 'Which city is the The Space Needle in? It is in Seattle, Washington, USA What city is the Eiffel Tower in? It is in Paris, France Which city is the Empire State Building in? It is in',\n",
       " 'The Space Needle is made of concrete. The Space Needle is the tallest man-made structure in the world. The Space Needle is one of the most photographed places in the world. The Space Needle',\n",
       " \"The Space Needle is in Seattle, WA (the city, not the needle). It's a very nice needle. I'm surprised that it's been around so long, since the Space Needle is a pretty famous landmark. I don\",\n",
       " 'The Statue of Liberty is located in the city of New York, United States.  The island, which was dedicated in 1886, stands in the harbor of New York and is a major tourist attraction in the city.  The statue is located at',\n",
       " 'Colosseum is located in the city of Rome in the region of Lazio, about  west of the centre of the city of Rome. It has been in use since the time of the Roman Emperor Vespasian, who constructed it to']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_weights(model, orig_weights)\n",
    "generate_fast(\n",
    "    mt = mt, \n",
    "    prompts = generation_prompts,\n",
    "    max_out_len = 50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
