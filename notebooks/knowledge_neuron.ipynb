{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.2+cu121', '4.36.2', '12.1')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import baukit\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "from src import functional\n",
    "import src.tokens as tokenization_utils\n",
    "\n",
    "torch.__version__, transformers.__version__, torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/relations/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# MODEL_PATH = \"EleutherAI/gpt-j-6B\"\n",
    "# MODEL_PATH = \"meta-llama/Llama-2-7b-hf\"\n",
    "# MODEL_PATH = \"mistralai/Mistral-7B-v0.1\"\n",
    "MODEL_PATH = \"state-spaces/mamba-2.8b-slimpj\" # state-spaces/mamba-2.8b\n",
    "\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_path=MODEL_PATH, \n",
    "    torch_dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cache promoting tokens for all the `down_proj` neurons\n",
    "# ---------------------------------------\n",
    "cut_off_rank = 50\n",
    "path = \"neuron_prommotions/out_proj\"\n",
    "out_proj_path_format = \"layers.{}.mixer.out_proj\"\n",
    "# ---------------------------------------\n",
    "\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# neuron_promotions = {layer_idx: {} for layer_idx in range(mt.n_layer)}\n",
    "\n",
    "# for layer_idx in tqdm(range(mt.n_layer)):\n",
    "#     print(f\"layer {layer_idx}\")\n",
    "#     out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer_idx))\n",
    "\n",
    "#     for column in tqdm(range(out_proj.weight.shape[1])):\n",
    "#         next_tok_candidates = functional.logit_lens(\n",
    "#             mt = mt, \n",
    "#             h = out_proj.weight[:, column],\n",
    "#             k = cut_off_rank\n",
    "#         )\n",
    "\n",
    "#         neuron_promotions[layer_idx][column] = [\n",
    "#             {\"token\": tok, \"logit\": logit} for tok, logit in next_tok_candidates\n",
    "#         ]\n",
    "    \n",
    "#     with open(os.path.join(path, f\"layer_{layer_idx}.json\"), \"w\") as f:\n",
    "#         json.dump(neuron_promotions[layer_idx], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_weights(mt):\n",
    "    weights_cached = {}\n",
    "    for layer in range(mt.n_layer):\n",
    "        out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer))\n",
    "        weights_cached[layer] = out_proj.weight.clone().detach()\n",
    "    return weights_cached\n",
    "\n",
    "def restore_weights(mt, weights_cached):\n",
    "    for layer in range(mt.n_layer):\n",
    "        out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer))\n",
    "        with torch.no_grad():\n",
    "            out_proj.weight[...] = weights_cached[layer]\n",
    "\n",
    "# weights_cached = cache_weights(mt)\n",
    "# restore_weights(mt, weights_cached)\n",
    "            \n",
    "####################################\n",
    "WEIGHTS_CACHED = cache_weights(mt)\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4382, ' computer'),\n",
       " (3694, ' software'),\n",
       " (16518, ' engineer'),\n",
       " (34513, ' programmer'),\n",
       " (13722, ' developer'),\n",
       " (48109, ' hacker')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################################\n",
    "# concepts = [\n",
    "#     \"doctor\", \"nurse\", \"therapist\",\n",
    "#     \"healthcare\", \"medicine\", \"medical\"\n",
    "# ]\n",
    "concepts = [\n",
    "    \"computer\", \"software\", \"engineer\", \"programmer\", \"developer\", \"hacker\"\n",
    "]\n",
    "########################################################################\n",
    "\n",
    "concept_start_token_ids= mt.tokenizer([\n",
    "    \" \" + concept + \" \" for concept in concepts\n",
    "], return_tensors=\"pt\", padding=True).input_ids[:, 0].tolist()\n",
    "\n",
    "[(id, mt.tokenizer.decode(id)) for id in concept_start_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_neuron_promotions(path, layer_idx):\n",
    "    with open(os.path.join(path, f\"layer_{layer_idx}.json\")) as f:\n",
    "        loaded_dict = json.load(f)\n",
    "        return {\n",
    "            int(k): v for k, v in loaded_dict.items()\n",
    "        }\n",
    "\n",
    "neuron_promotions = {layer_idx: {} for layer_idx in range(mt.n_layer)}\n",
    "for layer_idx in range(mt.n_layer):\n",
    "    neuron_promotions[layer_idx] = load_neuron_promotions(path, layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 neurons in layer 0 > [162]\n",
      "found 2 neurons in layer 1 > [909, 1673]\n",
      "found 2 neurons in layer 3 > [1099, 3128]\n",
      "found 3 neurons in layer 4 > [2397, 2500, 3044]\n",
      "found 5 neurons in layer 5 > [1807, 2817, 3633, 3905, 4807]\n",
      "found 3 neurons in layer 7 > [300, 668, 971]\n",
      "found 3 neurons in layer 8 > [1438, 2096, 2157]\n",
      "found 2 neurons in layer 10 > [3583, 3621]\n",
      "found 1 neurons in layer 11 > [3978]\n",
      "found 2 neurons in layer 12 > [277, 838]\n",
      "found 1 neurons in layer 13 > [1099]\n",
      "found 2 neurons in layer 14 > [2682, 3797]\n",
      "found 1 neurons in layer 16 > [5026]\n",
      "found 2 neurons in layer 17 > [39, 4357]\n",
      "found 3 neurons in layer 19 > [986, 2131, 4470]\n",
      "found 1 neurons in layer 22 > [250]\n",
      "found 1 neurons in layer 23 > [4009]\n",
      "found 2 neurons in layer 26 > [1789, 4829]\n",
      "found 2 neurons in layer 29 > [1467, 3219]\n",
      "found 1 neurons in layer 30 > [1806]\n",
      "found 2 neurons in layer 31 > [717, 1426]\n",
      "found 1 neurons in layer 33 > [2603]\n",
      "found 2 neurons in layer 34 > [2975, 3543]\n",
      "found 2 neurons in layer 36 > [3762, 4582]\n",
      "found 2 neurons in layer 37 > [3779, 3946]\n",
      "found 2 neurons in layer 39 > [651, 2803]\n",
      "found 2 neurons in layer 40 > [758, 2520]\n",
      "found 4 neurons in layer 41 > [1079, 2231, 2494, 4102]\n",
      "found 2 neurons in layer 42 > [2943, 4175]\n",
      "found 1 neurons in layer 43 > [243]\n",
      "found 9 neurons in layer 44 > [1951, 2998, 3135, 3284, 3557, 3913, 4037, 4340, 5113]\n",
      "found 3 neurons in layer 45 > [1494, 2408, 2560]\n",
      "found 3 neurons in layer 46 > [2072, 2099, 3035]\n",
      "found 1 neurons in layer 47 > [1463]\n",
      "found 3 neurons in layer 48 > [265, 1416, 1996]\n",
      "found 7 neurons in layer 49 > [112, 1523, 2041, 2967, 3470, 3594, 4427]\n",
      "found 4 neurons in layer 50 > [1966, 3369, 4308, 4714]\n",
      "found 4 neurons in layer 51 > [1535, 1714, 4394, 4459]\n",
      "found 2 neurons in layer 52 > [2640, 3592]\n",
      "found 6 neurons in layer 53 > [2193, 2245, 2863, 3774, 4513, 4913]\n",
      "found 7 neurons in layer 54 > [16, 2403, 3452, 3731, 3806, 3827, 4648]\n",
      "found 2 neurons in layer 55 > [671, 2179]\n",
      "found 3 neurons in layer 56 > [2806, 4058, 4986]\n",
      "found 11 neurons in layer 57 > [485, 842, 1181, 1905, 2703, 2858, 3307, 3417, 3557, 3864, 4261]\n",
      "found 6 neurons in layer 58 > [231, 905, 3514, 4021, 4132, 4267]\n",
      "found 12 neurons in layer 59 > [241, 264, 1267, 1338, 1506, 1893, 2599, 3344, 3977, 3982, 4821, 4910]\n",
      "found 3 neurons in layer 60 > [1501, 4064, 4858]\n",
      "found 2 neurons in layer 62 > [100, 2872]\n",
      "found 1 neurons in layer 63 > [1597]\n"
     ]
    }
   ],
   "source": [
    "# Cache promoting tokens for all the `down_proj` neurons\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "cut_off_rank = 1\n",
    "concept_drivers = {layer_idx: [] for layer_idx in range(mt.n_layer)}\n",
    "\n",
    "for layer_idx in range(mt.n_layer):\n",
    "    out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer_idx))\n",
    "\n",
    "    found_neurons = []\n",
    "\n",
    "    for column in range(out_proj.weight.shape[1]):\n",
    "        # for t in concept_start_token_ids:\n",
    "        #     if concept_ranks[t]['rank'] <= cut_off_rank:\n",
    "        #         concept_driver_neurons.append({\n",
    "        #             \"layer\": layer_idx,\n",
    "        #             \"neuron\": column,\n",
    "        #             \"concept_ranks\": concept_ranks,\n",
    "        #         })\n",
    "        #         break\n",
    "        candidate_tokens = [\n",
    "            candidate[\"token\"] for candidate in neuron_promotions[layer_idx][column]\n",
    "        ][:cut_off_rank]\n",
    "        for target_token in concepts:\n",
    "            found = False\n",
    "            for candidate in candidate_tokens:\n",
    "                if len(candidate.strip()) < 4:  # skip very short trivial tokens\n",
    "                    continue\n",
    "                if functional.is_nontrivial_prefix(\n",
    "                    prediction=candidate, target=target_token\n",
    "                ):\n",
    "                    found = True\n",
    "                    concept_drivers[layer_idx].append(column)\n",
    "                    found_neurons.append(column)\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "\n",
    "    if len(found_neurons) > 0:\n",
    "        print(\n",
    "            f\"found {len(found_neurons)} neurons in layer {layer_idx} > {found_neurons}\"\n",
    "        )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = 25\n",
    "# neuron = 1799\n",
    "\n",
    "\n",
    "# out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer))\n",
    "# logits = mt.lm_head(out_proj.weight[:, neuron])\n",
    "\n",
    "# logit_values = logits.sort(descending=True).values.detach().cpu().numpy()[:30]\n",
    "# logit_tokens = logits.sort(descending=True).indices.detach().cpu().numpy()[:30]\n",
    "\n",
    "# logit_tokens = ['\"{}\"'.format(mt.tokenizer.decode([t])) for t in logit_tokens]\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.bar(range(len(logit_values)), logit_values)\n",
    "# plt.xticks(range(len(logit_values)), logit_tokens, rotation=90)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' freelance', prob=0.019778123125433922),\n",
       "  PredictedToken(token=' nurse', prob=0.017641225829720497),\n",
       "  PredictedToken(token=' teacher', prob=0.014872241765260696),\n",
       "  PredictedToken(token=' professional', prob=0.013643579557538033),\n",
       "  PredictedToken(token=' reception', prob=0.0121234692633152)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! Don't forget to restore the weights\n",
    "restore_weights(mt, WEIGHTS_CACHED)\n",
    "\n",
    "prompt = \"Marina works as a\"\n",
    "prompt = tokenization_utils.maybe_prefix_eos(mt.tokenizer, prompt)\n",
    "\n",
    "functional.predict_next_token(\n",
    "    mt = mt,\n",
    "    prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Product', prob=0.21306917071342468),\n",
       "  PredictedToken(token=' Engineering', prob=0.07157208025455475),\n",
       "  PredictedToken(token=' Development', prob=0.04282478615641594),\n",
       "  PredictedToken(token=' product', prob=0.04239372909069061),\n",
       "  PredictedToken(token=' Group', prob=0.03424137085676193)]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magnify_scale = 100\n",
    "\n",
    "# ! Don't forget to restore the weights\n",
    "restore_weights(mt, WEIGHTS_CACHED)\n",
    "\n",
    "for layer in concept_drivers:\n",
    "    for neuron in concept_drivers[layer]:\n",
    "        out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer))\n",
    "        with torch.no_grad():\n",
    "            out_proj.weight[:, neuron] *= magnify_scale\n",
    "\n",
    "functional.predict_next_token(\n",
    "    mt = mt,\n",
    "    prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' freelance photographer and videographer. She has been working']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_weights(mt, WEIGHTS_CACHED)\n",
    "\n",
    "functional.mamba_generate(\n",
    "    mt = mt, \n",
    "    prompt = prompt,\n",
    "    topk=1\n",
    ").generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
