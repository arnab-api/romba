{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.2+cu121', '4.36.2', '12.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import baukit\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "from src import functional\n",
    "import src.tokens as tokenization_utils\n",
    "\n",
    "torch.__version__, transformers.__version__, torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/relations/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# MODEL_PATH = \"EleutherAI/gpt-j-6B\"\n",
    "# MODEL_PATH = \"meta-llama/Llama-2-7b-hf\"\n",
    "# MODEL_PATH = \"mistralai/Mistral-7B-v0.1\"\n",
    "MODEL_PATH = \"state-spaces/mamba-2.8b-slimpj\" # state-spaces/mamba-2.8b\n",
    "\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_path=MODEL_PATH, \n",
    "    torch_dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cache promoting tokens for all the `down_proj` neurons\n",
    "# ---------------------------------------\n",
    "cut_off_rank = 50\n",
    "path = \"../results/neuron_prommotions/out_proj\"\n",
    "out_proj_path_format = \"layers.{}.mixer.out_proj\"\n",
    "# ---------------------------------------\n",
    "\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# neuron_promotions = {layer_idx: {} for layer_idx in range(mt.n_layer)}\n",
    "\n",
    "# for layer_idx in tqdm(range(mt.n_layer)):\n",
    "#     print(f\"layer {layer_idx}\")\n",
    "#     out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer_idx))\n",
    "\n",
    "#     for column in tqdm(range(out_proj.weight.shape[1])):\n",
    "#         next_tok_candidates = functional.logit_lens(\n",
    "#             mt = mt, \n",
    "#             h = out_proj.weight[:, column],\n",
    "#             k = cut_off_rank\n",
    "#         )\n",
    "\n",
    "#         neuron_promotions[layer_idx][column] = [\n",
    "#             {\"token\": tok, \"logit\": logit} for tok, logit in next_tok_candidates\n",
    "#         ]\n",
    "    \n",
    "#     with open(os.path.join(path, f\"layer_{layer_idx}.json\"), \"w\") as f:\n",
    "#         json.dump(neuron_promotions[layer_idx], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_weights(mt):\n",
    "    weights_cached = {}\n",
    "    for layer in range(mt.n_layer):\n",
    "        out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer))\n",
    "        weights_cached[layer] = out_proj.weight.clone().detach()\n",
    "    return weights_cached\n",
    "\n",
    "def restore_weights(mt, weights_cached):\n",
    "    for layer in range(mt.n_layer):\n",
    "        out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer))\n",
    "        with torch.no_grad():\n",
    "            out_proj.weight[...] = weights_cached[layer]\n",
    "\n",
    "# weights_cached = cache_weights(mt)\n",
    "# restore_weights(mt, weights_cached)\n",
    "            \n",
    "####################################\n",
    "WEIGHTS_CACHED = cache_weights(mt)\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7345, ' doctor'),\n",
       " (15339, ' nurse'),\n",
       " (30286, ' therapist'),\n",
       " (11723, ' healthcare'),\n",
       " (9921, ' medicine'),\n",
       " (3739, ' medical')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################################\n",
    "concepts = [\n",
    "    \"doctor\", \"nurse\", \"therapist\",\n",
    "    \"healthcare\", \"medicine\", \"medical\"\n",
    "]\n",
    "# concepts = [\n",
    "#     \"computer\", \"software\", \"engineer\", \"programmer\", \"developer\", \"hacker\"\n",
    "# ]\n",
    "########################################################################\n",
    "\n",
    "concept_start_token_ids= mt.tokenizer([\n",
    "    \" \" + concept + \" \" for concept in concepts\n",
    "], return_tensors=\"pt\", padding=True).input_ids[:, 0].tolist()\n",
    "\n",
    "[(id, mt.tokenizer.decode(id)) for id in concept_start_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_neuron_promotions(path, layer_idx):\n",
    "    with open(os.path.join(path, f\"layer_{layer_idx}.json\")) as f:\n",
    "        loaded_dict = json.load(f)\n",
    "        return {\n",
    "            int(k): v for k, v in loaded_dict.items()\n",
    "        }\n",
    "\n",
    "neuron_promotions = {layer_idx: {} for layer_idx in range(mt.n_layer)}\n",
    "for layer_idx in range(mt.n_layer):\n",
    "    neuron_promotions[layer_idx] = load_neuron_promotions(path, layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2 neurons in layer 1 > [1218, 4799]\n",
      "found 4 neurons in layer 2 > [109, 428, 492, 1560]\n",
      "found 1 neurons in layer 3 > [3534]\n",
      "found 1 neurons in layer 4 > [2249]\n",
      "found 3 neurons in layer 5 > [269, 675, 4474]\n",
      "found 1 neurons in layer 6 > [4761]\n",
      "found 2 neurons in layer 7 > [1707, 2533]\n",
      "found 2 neurons in layer 10 > [2077, 4789]\n",
      "found 2 neurons in layer 12 > [314, 2858]\n",
      "found 1 neurons in layer 14 > [97]\n",
      "found 2 neurons in layer 15 > [3151, 3993]\n",
      "found 1 neurons in layer 18 > [3911]\n",
      "found 4 neurons in layer 19 > [1251, 1254, 2317, 3577]\n",
      "found 1 neurons in layer 24 > [3852]\n",
      "found 2 neurons in layer 25 > [1799, 2507]\n",
      "found 1 neurons in layer 26 > [1050]\n",
      "found 2 neurons in layer 27 > [1945, 4846]\n",
      "found 3 neurons in layer 28 > [418, 1999, 3322]\n",
      "found 1 neurons in layer 30 > [2819]\n",
      "found 2 neurons in layer 31 > [3021, 4491]\n",
      "found 3 neurons in layer 32 > [2065, 2158, 4184]\n",
      "found 2 neurons in layer 33 > [904, 3335]\n",
      "found 4 neurons in layer 37 > [208, 2105, 3376, 4177]\n",
      "found 1 neurons in layer 38 > [1383]\n",
      "found 2 neurons in layer 40 > [93, 436]\n",
      "found 1 neurons in layer 41 > [2338]\n",
      "found 3 neurons in layer 43 > [566, 689, 4923]\n",
      "found 6 neurons in layer 44 > [1127, 3202, 3431, 4484, 4528, 4879]\n",
      "found 3 neurons in layer 45 > [299, 1310, 1765]\n",
      "found 2 neurons in layer 46 > [2435, 2818]\n",
      "found 2 neurons in layer 47 > [136, 2366]\n",
      "found 1 neurons in layer 48 > [1928]\n",
      "found 2 neurons in layer 49 > [752, 3535]\n",
      "found 1 neurons in layer 50 > [111]\n",
      "found 4 neurons in layer 51 > [1457, 1947, 2623, 3132]\n",
      "found 3 neurons in layer 52 > [3899, 4310, 4849]\n",
      "found 5 neurons in layer 53 > [846, 2018, 2286, 2590, 4767]\n",
      "found 4 neurons in layer 54 > [1712, 2294, 3643, 4507]\n",
      "found 2 neurons in layer 55 > [2257, 4908]\n",
      "found 1 neurons in layer 56 > [3238]\n",
      "found 3 neurons in layer 58 > [569, 3130, 3301]\n",
      "found 2 neurons in layer 59 > [964, 1275]\n",
      "found 3 neurons in layer 60 > [474, 1560, 3048]\n",
      "found 2 neurons in layer 61 > [813, 2696]\n",
      "found 1 neurons in layer 62 > [2995]\n",
      "found 1 neurons in layer 63 > [1060]\n"
     ]
    }
   ],
   "source": [
    "# Cache promoting tokens for all the `down_proj` neurons\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "cut_off_rank = 1\n",
    "concept_drivers = {layer_idx: [] for layer_idx in range(mt.n_layer)}\n",
    "\n",
    "for layer_idx in range(mt.n_layer):\n",
    "    out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer_idx))\n",
    "\n",
    "    found_neurons = []\n",
    "\n",
    "    for column in range(out_proj.weight.shape[1]):\n",
    "        # for t in concept_start_token_ids:\n",
    "        #     if concept_ranks[t]['rank'] <= cut_off_rank:\n",
    "        #         concept_driver_neurons.append({\n",
    "        #             \"layer\": layer_idx,\n",
    "        #             \"neuron\": column,\n",
    "        #             \"concept_ranks\": concept_ranks,\n",
    "        #         })\n",
    "        #         break\n",
    "        candidate_tokens = [\n",
    "            candidate[\"token\"] for candidate in neuron_promotions[layer_idx][column]\n",
    "        ][:cut_off_rank]\n",
    "        for target_token in concepts:\n",
    "            found = False\n",
    "            for candidate in candidate_tokens:\n",
    "                if len(candidate.strip()) < 4:  # skip very short trivial tokens\n",
    "                    continue\n",
    "                if functional.is_nontrivial_prefix(\n",
    "                    prediction=candidate, target=target_token\n",
    "                ):\n",
    "                    found = True\n",
    "                    concept_drivers[layer_idx].append(column)\n",
    "                    found_neurons.append(column)\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "\n",
    "    if len(found_neurons) > 0:\n",
    "        print(\n",
    "            f\"found {len(found_neurons)} neurons in layer {layer_idx} > {found_neurons}\"\n",
    "        )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = 25\n",
    "# neuron = 1799\n",
    "\n",
    "\n",
    "# out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer))\n",
    "# logits = mt.lm_head(out_proj.weight[:, neuron])\n",
    "\n",
    "# logit_values = logits.sort(descending=True).values.detach().cpu().numpy()[:30]\n",
    "# logit_tokens = logits.sort(descending=True).indices.detach().cpu().numpy()[:30]\n",
    "\n",
    "# logit_tokens = ['\"{}\"'.format(mt.tokenizer.decode([t])) for t in logit_tokens]\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.bar(range(len(logit_values)), logit_values)\n",
    "# plt.xticks(range(len(logit_values)), logit_tokens, rotation=90)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' freelance', prob=0.027900464832782745),\n",
       "  PredictedToken(token=' software', prob=0.020736632868647575),\n",
       "  PredictedToken(token=' consultant', prob=0.016052845865488052),\n",
       "  PredictedToken(token=' full', prob=0.015277174301445484),\n",
       "  PredictedToken(token=' Senior', prob=0.01317055243998766)]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! Don't forget to restore the weights\n",
    "restore_weights(mt, WEIGHTS_CACHED)\n",
    "\n",
    "prompt = \"Eric works as a\"\n",
    "prompt = tokenization_utils.maybe_prefix_eos(mt.tokenizer, prompt)\n",
    "\n",
    "functional.predict_next_token(\n",
    "    mt = mt,\n",
    "    prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' health', prob=0.9831162095069885),\n",
       "  PredictedToken(token=' healthcare', prob=0.01635204441845417),\n",
       "  PredictedToken(token=' Health', prob=0.0002684734936337918),\n",
       "  PredictedToken(token=' care', prob=0.0001469957787776366),\n",
       "  PredictedToken(token=' Healthcare', prob=8.562780567444861e-05)]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magnify_scale = 100\n",
    "\n",
    "# ! Don't forget to restore the weights\n",
    "restore_weights(mt, WEIGHTS_CACHED)\n",
    "\n",
    "for layer in concept_drivers:\n",
    "    for neuron in concept_drivers[layer]:\n",
    "        out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer))\n",
    "        with torch.no_grad():\n",
    "            out_proj.weight[:, neuron] *= magnify_scale\n",
    "\n",
    "functional.predict_next_token(\n",
    "    mt = mt,\n",
    "    prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' freelance writer and editor. He has written for a']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_weights(mt, WEIGHTS_CACHED)\n",
    "\n",
    "functional.mamba_generate(\n",
    "    mt = mt, \n",
    "    prompt = prompt,\n",
    "    topk=1\n",
    ").generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driving LM by replacing certain neurons with a set random concept directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7345, ' doctor'),\n",
       " (15339, ' nurse'),\n",
       " (30286, ' therapist'),\n",
       " (11723, ' healthcare'),\n",
       " (9921, ' medicine'),\n",
       " (3739, ' medical')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(id, mt.tokenizer.decode(id)) for id in concept_start_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50280, 2560])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head = baukit.get_module(mt.model, \"lm_head\")\n",
    "lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import models\n",
    "\n",
    "noise_level = .1\n",
    "num_rand_vectors = 10\n",
    "\n",
    "random_neurons = []\n",
    "\n",
    "for id in concept_start_token_ids:\n",
    "    unembed_row = lm_head.weight[id].squeeze().clone().detach()\n",
    "    for _ in range(num_rand_vectors):\n",
    "        random_neurons.append(unembed_row + torch.randn_like(unembed_row) * noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "all_neurons = [\n",
    "    (layer, neuron) for layer in range(mt.n_layer) for neuron in range(lm_head.weight.shape[1])\n",
    "]\n",
    "\n",
    "random_neuron_idxes = random.sample(all_neurons, k=len(random_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' healthcare', prob=0.9999998807907104),\n",
       "  PredictedToken(token=' Healthcare', prob=1.119929962101196e-07),\n",
       "  PredictedToken(token=' nurse', prob=8.797612838966698e-15),\n",
       "  PredictedToken(token='health', prob=1.6366389135693312e-16),\n",
       "  PredictedToken(token=' nurses', prob=8.44718826772622e-17)]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magnify_scale = 100\n",
    "\n",
    "# ! Don't forget to restore the weights\n",
    "restore_weights(mt, WEIGHTS_CACHED)\n",
    "\n",
    "for (layer, neuron_idx), neuron in zip(random_neuron_idxes, random_neurons):\n",
    "    out_proj = baukit.get_module(mt.model, out_proj_path_format.format(layer))\n",
    "    with torch.no_grad():\n",
    "        out_proj.weight[:, neuron_idx] = magnify_scale * neuron\n",
    "\n",
    "functional.predict_next_token(\n",
    "    mt = mt,\n",
    "    prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
